LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping
===

## Abstract
- 우리는 매우 정확한 실시간 모바일 로봇 궤적 추정 및 지도 구축을 달성하는 스무딩 및 매핑을 통한 긴밀하게 tightly-coupled된 lidar inertial odometry 측정을 위한 프레임워크인 LIO-SAM을 제안
- LIO-SAM은 factor graph위에 lidar-inertial odometry를 공식화하고 loop closure을 포함한 다수의 상대 및 절대 측정값을 다양한 소스에서 시스템의 factor로 통합할 수 있음
- IMU pre-integration에서 추정된 모션은 포인트 클라우드의 기울기를 줄이고(de-skews) 라이더 odometry 최적화를 위한 initial guess 생성
- 얻어진 lidar odometry 솔루션은 IMU의 바이어스를 추정하는 데 사용
  - IMU bias : 입력 값에서 출력 값의 일정한 오프셋
- 실시간으로 고성능을 보장하기 위해 우리는 라이더 스캔을 global map에 일치시키는 대신 포즈 최적화를 위해 오래된 라이더 스캔을 marginalize 
- 글로벌 스케일 대신 로컬 스케일에서 스캔 매칭은 시스템의 실시간 성능을 크게 향상
- 키프레임을 선택적으로 도입하고 새로운 키프레임을 이전 "하위 키프레임"의 고정 크기 세트에 등록하는 효율적인 슬라이딩 윈도우 접근 방식도 마찬가지

## 1. Introduction
### - 기본이론 (왜 라이다를 쓰는지)
- State estimation, localization and mapping은 다른 많은 기능 중에서 피드백 제어, 장애물 회피 및 계획에 필요한 성공적인 지능형 모바일 로봇의 기본 전제 조건
- 비전 기반 방법은 일반적으로 단안 또는 스테레오 카메라를 사용하고 연속 이미지에 걸쳐 특징을 삼각 측량하여 카메라 동작을 결정
- 비전 기반 방법은 장소 인식에 특히 적합하지만 초기화, 조명 및 범위에 대한 감도로 인해 자율 내비게이션 시스템을 지원하는 데 단독으로 사용될 경우 신뢰할 수 없음
- 반면에 라이다 기반 방법은 조명 변화에 크게 영향을 받지 않
  - LeGO-LOAM과 똑같은 말
- 특히 최근 Velodyne VLS-128 및 Ouster OS1-128과 같은 장거리 고해상도 3D 라이더가 출시됨에 따라 라이더는 3D 공간에서 환경의 미세한 세부 사항을 직접 캡처하는 데 더 적합
---
### - LOAM
- LOAM은 data를 global voxel map에 저장함으로써 loop detection을 수행하는 것이 어려움
- 또한, pose correction을 위해 GPS와 같은 다른 절대 측정을 통합하기 어려움
  - 🧠️ 걍 loop closure detection안되고, GPS추가 안된다는 말인듯 🧠️
- 이러한 voxel map이 feature가 풍부한 환경에서 조밀해지면(dense=밀도가 높아지면) 온라인 최적화 프로세스의 효율이 줄어듬
- LOAM은 또한, 스캔 매칭 기반 방법이 핵심이므로 large-scale test에서 drift 발생
---
### - 🌟️ This paper proposed, ~ 🌟️
- smoothing and mapping, LIO-SAM을 통해 tightly-coupled lidar inertial odometry 측정을 위한 프레임워크 제안
#### - sensor motion to estimated의 활용 1. point cloud de-skewing 2. lidar odometry optimization의 초기값
- raw IMU 측정을 사용하여 라이다 스캔 중 센서 모션을 추정하는 point cloud de-skew를 위한 non-linear motion model을 가정
- de-skewing point cloud 외에도, 추정된 모션은 lidar odometry optimization을 위한 initial guess로 사용 (초기값)
- 그런 다음 얻은 lidar odometry 측정 솔류션을 사용하여 factor graph에서 얻은 IMU의 bias를 추정
- 로봇 궤적 추정을 위한 global factor graph를 도입함으로써, 우리는 lidar 및 IMU 측정을 사용하여 센서 융합을 효율적으로 수행하고 로봇 포즈 간의 장소 인식을 통합
- 또한, 가능한 경우 GPS 위치 및 나침반 방향과 같은 절대 측정을 도입할 수 있음
- 다양한 소스의 factor의 모음은 graph의 공동 optimization에 사용
- 또한, LOAM과 같은 global map에 스캔을 일치시키는 대신 포즈 최적화를 위해 오래된 라이더 스캔을 marginalize
- 글로벌 스케일 대신 로컬 스케일에서 스캔 매칭은 시스템의 실시간 성능을 크게 향상
- 키프레임을 선택적으로 도입하고 새로운 키프레임을 이전 "하위 키프레임"의 고정 크기 세트에 등록하는 효율적인 슬라이딩 윈도우 접근 방식도 마찬가지
---
### - 요약하자면,
- 다중 센서 융합 및 전역 최적화에 적합한 팩터 그래프 위에 구축된 긴밀하게 연결된 라이더 관성 주행 거리 측정 프레임워크
- 선택적으로 선택된 새 키프레임을 고정 크기의 이전 하위 키프레임 세트에 등록하여 실시간 성능을 가능하게 하는 효율적인 로컬 슬라이딩 윈도우 기반 스캔 일치 접근 방식
- 제안된 프레임워크는 다양한 규모, 차량 및 환경에 대한 테스트를 통해 광범위하게 검증

## 2. Related work
### - 현재 기본 동향
- Lidar odometry는 일반적으로 ICP 및 GICP와 같은 스캔 매칭 방법을 사용하여 두 개의 연속 프레임 간의 상대적 변환을 찾는 방식으로 수행
- 전체 포인트 클라우드를 매칭하는 대신 feature 기반 매칭 방법은 계산 효율성으로 인해 인기 있는 대안이 됨
- 구조화된 환경에서 작업을 가정하면 포인트 클라우드에서 평면을 추출하고 최소 제곱 문제를 해결하여 일치
---
### - Lidar 문제점과 해결방안
- 그러나 최신 3D 라이더의 회전 메커니즘과 센서 동작으로 인해 스캔의 포인트 클라우드가 종종 왜곡(skewed)
- 기울어진 포인트 클라우드 또는 feature을 사용하여 registration하면 결국 큰 드리프트가 발생하므로 포즈 추정에 라이다만 사용하는 것은 이상적이지 않음
- 따라서 Lidar는 일반적으로 상태 추정 및 매핑을 위해 IMU 및 GPS와 같은 다른 센서와 함께 사용
- 센서 융합을 활용하는 이러한 설계 체계는 일반적으로 느슨하게 결합된 융합과 긴밀하게 결합된 융합의 두 가지 범주로 그룹화할 수 있음
#### 👉️ loosely-coupled
##### - LOAM
- LOAM에서 IMU는 라이더 스캔의 기울기를 줄이고 스캔 일치를 위한 동작을 제공하기 위해 도입
- 그러나 IMU는 알고리즘의 최적화 프로세스에 관여하지 않음
- 따라서 LOAM은 느슨하게 결합된 방법으로 분류
##### - LeGO-LOAM
- 경량의 지상 최적화 라이더 주행 거리 측정 및 매핑(LeGO-LOAM) 방법은 지상 차량 매핑 작업에 대해 제안
- IMU 측정의 융합은 LOAM과 동일
- 느슨하게 결합된 융합에 대한 보다 대중적인 접근 방식은 확장 칼만 필터(EKF)를 사용하는 것
#### 👉️ tightly-coupled
##### - IN2LAMA 
- 밀접하게 결합된 시스템은 일반적으로 향상된 정확도를 제공하며 현재 진행 중인 연구의 주요 초점
- pre-integration된 IMU 측정은 디스큐 포인트 클라우드에 활용
##### - R-LINS
- error-state Kalman filter 를 이용하여 tightly-coupled 방식으로 recursive 하게 robot 의 state estimate 를 correct 함
##### - LIOM
- LiDAR 와 IMU 를 이용하여 얻어진 factor 들을 jointly optimize 하여 LOAM 보다 나은 성능을 보임
- but, 모든 sensor measurement 를 처리하는것으로 디자인 되어 있기 때문에 real-time 으로 수행이 어려움

## 3. LiDAR Inertial Odometry Via Smoothing and Mapping
### 3-A. System Overview
- 먼저 논문 전체에서 사용하는 프레임과 표기법을 정
- 월드 프레임을 W로, 로봇 바디 프레임을 B로 표시
- 또한 편의상 IMU 프레임이 로봇 바디 프레임과 일치한다고 가정
- 로봇 상태 x는 아래와 같음  

![image](https://user-images.githubusercontent.com/108650199/203208802-31e40c02-8e8d-4df2-a4f1-9620e4c6ec18.png) ![image](https://user-images.githubusercontent.com/108650199/203208869-79100b30-0e37-4732-9ca2-00db389026ac.png)

- B에서 W로의 T∈SE(3)은 T = [R | P] 로 나타냄
---
#### - Fig 설명

![image](https://user-images.githubusercontent.com/108650199/203209275-dfb67783-4222-475b-a726-bcca7fb0ab53.png)

- 시스템은 3D 라이더, IMU 및 선택적으로 GPS로부터 센서 데이터를 수신
- 우리는 이러한 센서의 관찰을 사용하여 로봇의 상태와 궤적을 추정하려고 함
- 이 상태 추정 문제는 최대 사후(MAP) 문제로 공식화될 수 있음
- Bayes net과 비교할 때 추론을 수행하는 데 더 적합하기 때문에 factor graph를 사용하여 이 문제를 모델링
- 가우스 잡음 모델을 가정하면 우리 문제에 대한 MAP 추론은 비선형 최소제곱 문제를 푸는 것과 같음
- 일반성을 잃지 않고 제안된 시스템은 고도계의 고도 또는 나침반의 방향과 같은 다른 센서의 측정을 통합할 수도 있음
---
#### - factor graph 구성
- 구성을 위한 하나의 변수 유형과 함께 4가지 유형의 f 액터를 소개
- 특정 시간의 로봇 상태를 나타내는 이 변수는 그래프의 노드에 기인
- 네 가지 유형의 요인은 (a) IMU 사전 통합 factor
  - (b) 라이더 주행 거리 측정 factor
  - (c) GPS factor
  - (d) 루프 폐쇄 factor
- 로봇 자세의 변화가 사용자 정의 임계값(threshold)을 초과하면 새로운 로봇 상태 노드 x가 그래프에 추가
- factor graph는 베이즈 트리(iSAM2)를 사용한 incremental smoothing and mapping을 사용하여 새 노드를 삽입할 때 최적화

### 3-B. IMU Preintegration Factor
- IMU의 각속도와 가속도 측정의 정의

![image](https://user-images.githubusercontent.com/108650199/203223497-d3a669ef-8150-4481-b9b7-9206e8809c6a.png)

  - 여기서 b : 천천히 변화하는 bias, n : whith noise, R : rotation matrix (Rbw : W에서 B의 R), g : W에서의 중력 벡터
- 우리는 또한 IMU로부터의 측정을 사용해서 로봇의 motionㅇ르 측정할 수 있음 
- t+델타t만큼 시간의 로봇의 속도, 위치, roation을 계산하면 아래와 같음

![image](https://user-images.githubusercontent.com/108650199/203223879-7f64c527-bb64-47f4-9b33-2130d49e03bc.png)
  
  - 왜냐면 RRt=I 이므로 맨 아래와 같은 조건이 성립
- 여기서, 우리는 적분동안 B의 각속도와 가속도가 끊임없이 유지되는 것을 볼 수 있음
- 이후, IMU preintegration method를 2개의 timestamp 사이의 상대적인 body motion을 얻기 위해 어떤 논문을 참조하여 적용했음

![image](https://user-images.githubusercontent.com/108650199/203224163-41680625-1e60-469f-b858-38d0f08564d9.png)

- IMU preintegration을 적용함으로써 자연스럽게 factor graph의 제약 조건(IMU preintegration factor) 중 하나가 됨
- IMU bias는 graph에서 lidar odometry factor와 함께 같이 optimization됨

### 3-C. Lidar Odometry Factor
#### - 이전의 feature 뽑는 작업 (ex:LeGO-LOAM)
- 새로운 lidar scan이 오면, 먼저 feature extration을 수행
- Edge와 planar feature은 local 영역에대해 point의 거칠기(c)를 평가함으로써 추출됨 
  - c max : edge, c min : planar
- 시간 i에서 lidar scan에서 추출된 edge, planar feature을 각각 Fe, Fp로 표시
- 모든 feature 나타낸건 그 F두개 겹쳐진 그표시 ㅇㅇ
- 이러한 FF는 B(로봇 바디 프레임)에서 표시됨 
---
#### - 여기서 주장하는 바
- 모든 라이다 프레임을 사용하여 계산하고 그래프에 요소를 추가하는 것은 계산하기 어렵기 때문에 시각적 SLAM 분야에서 널리 사용되는 키프레임 선택 개념을 채택
- 결정 방법
  - 로봇 pose 변화시, 이전 상태 xi와 비교하여 사용자가 지정한 threshold를 초과하면 lidar frame FFi+1을 선택
    - 이 keyframe 사이의 lidar frame 다 버림
  - 새로 저장한 keyframe FFi+1은 factor graph의 새 로봇 상태 node와 연관
- 장점
  - 이렇게 keyframe 정하면, 맵 밀도와 메모리 소비 간의 균형을 맞춰줌
  - 실시간의 non-linear optimization에 적합한 상대적으로 sparse factor를 유지하는데 도움이 됨
- threshold 지정
  - position : 1m
  - roation : 10도
- lidar odometry factor의 생성은 아래의 step을 따름
#### 1) Sub-keyframes for voxel map 
- sliding window approach를 사용해서 최근 lidar scan의 고정된 수를 포함하는 point cloud map을 생성
- sub-keyframe : tf를 최적화하는 대신 연속 lidar scan 사이의 가장 최근 keyframe n개를 추출 
- sub-keyframe은 tf를 사용해서 W frame으로 tf
- tf된 sub-keyframe은 voxel map Mi와 합병됨

![image](https://user-images.githubusercontent.com/108650199/203236825-544ebbc7-f3a1-41a9-a73e-b2c907427669.png)

- ![image](https://user-images.githubusercontent.com/108650199/203236887-d513561e-6993-45bd-aada-8b121aae5aaf.png) 은 동일한 voxel cell에 속하는 중복 feature 제거하기 위해 downsampling 실시

#### 2) scan matching
- 우리는 ![image](https://user-images.githubusercontent.com/108650199/203237517-cb89d6db-08e8-40ce-97c6-e573f5ef4368.png) 이기도 한 새로 얻은 라이더 프레임 ![image](https://user-images.githubusercontent.com/108650199/203237555-354bc406-9857-4313-8aca-67f721a09920.png) 를 일치시킴
- 다른 논문에서 쓰던걸 들고오는데 왜냐면, 다양한 도전적인 환경에서 계산효율성과 강건함 때문
  - 초기 tf는 IMU pre-integration을 통해 얻어진 값을 사용
  - B에서 W로 tf된 feature edge, planar에서 ('F) M와의 대응관계 얻을 수 있음

#### 3) relative transformation
- feature과 edge, feature과 planar사이의 거리는 아래 식으로 정의

![image](https://user-images.githubusercontent.com/108650199/203238152-bf5eb160-ff5f-45ea-bd72-9640bdf72973.png)

- 이러한 거리를 GaussNewton method를 사용해 optimization 진행

![image](https://user-images.githubusercontent.com/108650199/203238200-e5b212d4-7582-4bdd-b971-253cf9eaef0e.png)

- 마지막으로, Ti+1로 최적화를 진행하면 우리는 상대 tf를 얻을 수 있음
- 이는 xi와 xi+1사이의 상대 tf임 

![image](https://user-images.githubusercontent.com/108650199/203238213-ec9e1604-b9f8-4a07-8587-e85f9698e87a.png)

### 3-D. GPS Factor
- IMU preintegration과 lidar odometry factor를 활용해서 충분히 신뢰할 수 있는 state estimation을 할 수 있지만,
- long~ duration navigation tasks일 때, drift 발생
- 이는 GPS와 같은 절대 측정을 사용해서 drift를 없앰
#### - 방법
- GPS를 local 데카르트 좌표계로 tf
- GPS factor를 factor graph에 new node로 추가
- GPS 신호가 라이다 frame과 하드웨어 동기화하지 않으면 라이다 frame의 시간을 기반으로 GPS 측정값을 선형보간함
- GPS 신호는 계속 받을 필요가 없음 
  - 왜냐하면 drift가 매우 slowly하게 일어나기 때문
- 따라서, estimated position covariance > received GPS position covariance 일 때만 받음

### 3-E. Loop Closure Factor
- 간단하지만 효율적인 유클라디안 거리기반 loop closure detection approach를 사용
- 본 framework는 다른 loop closure method와 호환가능
#### - 방법
- 새로운 상태 xi+1가 factor graph에 추가되면 그래프에서 가장 가까운 이전 상태를 찾음
- x3이 젤 가깝다고 예를 들자면,
- 그에 대한 sub-keyframe을 찾아 FF(라이다 프레임으로 edge와 planar로 구성)와 scan matching 진행
  - 스캔 매칭전에 W로 tf 미리 시켜놔야함 ㅇㅇ 전제조건
- 그렇게하면 우리는 realative tf인 델타T3를 얻어 loop closure factor로 저장가능
- 여기서 루프클로저 인덱스 m은 12, search distance 15m로 정의 
#### - 이점
- 실제로, loop closure factor은 GPS와 같은 절대 센서를 사용할 때, 로봇의 고도 drift를 보정할 때 매우 유용함
- 왜냐면 GPS 고도는 매우 부정확하기 때문에 loop closure은 매우 효과적
