> [Reference](https://alida.tistory.com/44)

## Abstract
- 우리는 6자유도에서 움직이는 2축 라이더의 범위 측정을 사용하여 주행 거리 측정 및 매핑을 위한 실시간 방법을 제안
- 범위 측정이 다른 시간에 수신되고 모션 추정의 오류로 인해 결과 포인트 클라우드가 잘못 registration 될 수 있기 때문에 문제가 어려움
- 현재까지 일관된 3D 지도는 오프라인 배치 방법으로 구축할 수 있으며, 종종 시간 경과에 따른 드리프트를 수정하기 위해 loop closure를 사용
- 우리의 방법은 높은 정확도 범위 또는 관성 측정 없이도 낮은 드리프트와 낮은 계산 복잡성을 모두 달성
- 🌟️ 핵심 아이디어는 1. 동시에 많은 수의 변수를 최적화
  - 2.복잡한 문제인 simultaneous localization 및 mapping 
  - 이렇게 두 개의 알고리즘으로 나누는 것 🌟️
- 한 알고리즘은 라이더의 속도를 추정하기 위해 높은 주파수이지만 lower-fidelity(낮은 충실도=저품질)에서 주행 거리 측정을 수행
- 또 다른 알고리즘은 포인트 클라우드의 정밀 매칭 및 등록을 위해 훨씬 낮은 빈도로 실행
- 두 알고리즘을 결합하면 실시간으로 메서드를 매핑

## 1. Introduction
- 라이다를 사용한 매핑은 측정된 거리에 관계없이 오류가 상대적으로 일정한 고주파수 범위 측정을 제공할 수 있기 때문에 일반적
- 라이다의 유일한 동작이 레이저빔을 회전시키는 것이기 때문에 포인트 클라우드의 registration은 간단
- 라이더 자체가 움직이는 경우 정확한 매핑을 위해서는, 연속적인 레이저 거리 측정 중 라이더 pose에 대한 지식이 필요
  - 해결 방법 1 : 고정 좌표계에 레이저 포인트를 등록하기 위해 독립적인 위치 추정 (ex : gps/ins)를 사용
  - 해결 방법 2 : 레이저 포인터를 등록하기 위해 wheel encoder또는 visual odometry와 같은 odometry measurement를 사용
    - but, odometry measurement는 시간이 지남에 따라 drift 발생하므로 drift 감소 (ex : loop closure)에 많은 관심을 기울임 
- 본 논문에서는 6자유도에서 움직이는 2축 라이더를 사용하여 낮은 드리프트 주행 거리로 지도를 만드는 경우를 고려
  - 💥️ 본 논문의 방법은 odometry estimation의 drift minimizing과 관련된 문제를 푸시하기 위한 것으로 현재 loop closure을 포함 X 💥️
- 불필요하지만 IMU 사용할 수 있는 경우, 고주파 동작을 설명하는데 도움이 되도록 motion을 사전에 제공
- 특히, 두 알고리즘 모두 sharp한 모서리와 평면 표면에 있는 특징점을 추출하고 특징점을 edge line segments 및 평면 표면 패치에 각각 일치
  
  ![image](https://user-images.githubusercontent.com/108650199/202098955-43378687-b89d-4f3f-ae3d-2a8e0c264b68.png)

  - edge line segments : edge line을 분할
- odometry 알고리즘에서는 빠른 계산을 보장하여 특징점의 대응 관계를 찾음
- mapping 알고리즘에서 correspondences는 연관된 고유값 및 고유 벡터를 통해 로컬 포인트 클러스터의 기하학적 분포를 검사하여 결정
  - 또한, mapping은 고정밀 움직임 추정 및 맵을 생서아기 위해 배치 최적화(ICP와 비슷)로 수행
- 병렬 알고리즘 구조는 실시간으로 해결해야 할 문제의 실행 가능성을 보장
- 또한, 움직임 추정은 더 높은 주파수에서 수행되기 때문에 mapping은 정확성을 강화하기 위해 충분한 시간이 주어짐
- lower frequency로 실행할 때 매핑 알고리즘은 많은 수의 특징점을 통합할 수 있고 수렴을 위해 충분히 많은 반복을 사용할 수 있음

## 5-A. LOAM feature point extration
- LIDAR cloud pk에서 feature point를 extration
- scan plane의 points들은 0.25° resolution을 가짐
  - 왜 resolution의 단위가 °냐면 레이저스캔을 돌면서 측정하는데 아무리 연속적으로 보인다해도 보낼때 사이의 각도가 존재..
  - 당연히 이게 좁을수록 데이터를 조밀하게 받을 수 있음
- 180°/s, 40Hzfh 돌아서 수직 방향으로는 4.5° resolution을 가짐 (각 채널이 크면 더 높겠쥬)
- pk에서의 feature point extration은 co-planar(같은 평면)안의 개별적인 scan 정보만 사용
- 우린 여기서 sharp한 edge와 planar surface patch(평면 표면 부분) 에 있는 feature point로 설정
- Pk안의 point를 i라고 하면, s는 동일 스캔에서 laser scanner에 의해 반환된 i의 연속적인 point set
  - 레이저는 cw나 ccw방향으로 도니까 s는 동일하게 반으로 쪼개면 i/2개의 point를 갖고있고 두 point사이의 간격은 0.25°
    - 왜냐면 수평 resolution은 0.25°라서

#### - local surface의 smoothness 평가하는 용어 정의

![image](https://user-images.githubusercontent.com/108650199/196868820-a2c69e34-3b1c-4cd2-a1f7-09205289c59f.png)

- scan point i는 c값을 기준으로 정의
  - Max C : edge point
  - Min C : planar point
- 위와 같은 방법으로 feature point가 선택됨
- 또한, 환경에서 feature point 고르게 분포시킬려고 scan을 하위 4개로 또 쪼갬
  - 각 쪼개진 하위 scan 1개당 Max 2 edge, Max 4 planar 제공
- 그리고, i (Pk의 point)는 C값이 threshold보다 크거나 작으면 버려짐  

> 예외 정의

![image](https://user-images.githubusercontent.com/108650199/196868878-c64b6a4f-a5bd-4cd5-80e5-7a03c2c011b1.png)

##### - In, 그림 A
- 레이저 빔에 대해 평행한 local planar surface point는 신뢰 X

##### - In, 그림 B
- 가려진 영역의 경계에 있는 point도 신뢰 X
  - 왜냐하면 여기서 볼 땐 B때매 가려져서 A 오른쪽의 점들이 안보이는데 cw방향으로 돌다가 다른 scan에서 보면 가려진 영역이 보일수도 있기 때문 > 혼동
- 그렇게 B처럼 가까운 점이 i가 됨
- 이러한 기준을 세워 point i의 집합 S를 정의  

> 위의 방식대로 feature point 추출한 결과값
 
![image](https://user-images.githubusercontent.com/108650199/196868870-326bc3dc-fb88-4481-9337-737bd69bc0f0.png)

- yellow point : edge
- red point : planar


## 5-B. LOAM Feature Point Correspondence

![image](https://user-images.githubusercontent.com/108650199/198930419-d386ee9f-1d70-4657-bdbf-cec3f92ca27d.png)

- 오도메트리는 sweep내에서의 LiDAR의 움직임을 추정
  - tk를 sweep k의 시작 시간이라고 정의
  - 각 sweep의 끝에서 sweep동안 감지된 포인트 클라우드 Pk는 타임 스탬프 tk+1로 재투영
  - 재투영된 포인트 클라우드를 ̄Pk로 표시
  - 다음 스윕 k+1 동안 ̄Pk는 새로 수신된 포인트 클라우드 Pk+1과 함께 사용되어 라이더의 움직임을 추정
- 지금은 ̄Pk와 Pk+1을 모두 사용할 수 있다고 가정하고 두 라이더 클라우드 간의 대응 관계를 찾는 것으로 시작
- Pk+1을 사용하면 라이다 클라우드에서 에지 포인트와 평면 포인트를 찾을 수 있음

![image](https://user-images.githubusercontent.com/108650199/198934644-9a4d35a9-bdf3-4a78-b68e-6e32edbb0bbe.png)

- Ek+1과 Hk+1을 각각 edge point와 planar point의 집합이라고 하자
- Ek+1의 점에 대한 대응으로 ̄Pk에서 에지 라인을 찾고 Hk+1의 점에 대한 대응으로 평면 패치를 찾음
- 스윕 k +1의 시작 부분에서 Pk+1은 비어 있는 집합이며 스윕 과정에서 더 많은 포인트가 수신됨에 따라 증가
  - 라이더 오도메트리는 스위프 동안 6-DOF 움직임을 재귀적으로 추정하고 Pk+1이 증가함에 따라 점차적으로 더 많은 포인트를 포함
- 각 반복에서 Ek+1 및 Hk+1은 현재 추정된 변환을 사용하여 스윕의 시작 부분으로 재투영
- E-k+1과 ̃Hk+1을 재투영된 점 집합이라고 정의

![image](https://user-images.githubusercontent.com/108650199/198934837-4b19c326-79e3-4394-8ebc-87c49563b5da.png)

  - E-k+1 및 ̃Hk+1의 각 점에 대해 ̄Pk에서 가장 가까운 이웃 점을 찾음
  - 여기서 ̄Pk는 빠른 인덱스를 위해 3D KD-tree에 저장

![image](https://user-images.githubusercontent.com/108650199/198934918-e0bab9e6-1a05-48b5-b28c-dc648eb4e9de.png)


![image](https://user-images.githubusercontent.com/108650199/196875624-217f5a30-5a3a-414e-bd5e-e080c8993011.png)

- 𝑃_(𝑘+1) 에서 찾은 edge와 planar를 𝐸_(𝑘+1), 𝐻_(𝑘+1) 이라 하면 이 point들에 대해 𝑃_𝑘 에 있는 point중 nearest point를 찾아 correspondence 생성

#### - Edge point

![image](https://user-images.githubusercontent.com/108650199/196875723-50b3bfe2-6c56-49aa-996e-e91839d9b69d.png)

- 𝑃_𝑘 nearest point를 j, 로 j의 consecutive point l ∈ 𝑃_𝑘를 구함
- j,l의 smoothness 계산해 둘 다 edge라면 (j,l)edge line과 I 사이의 correspondenc만들고, 옆의 식과 같이 distatnce를 구함

#### - Planar point

![image](https://user-images.githubusercontent.com/108650199/196875737-84dc1265-2e31-4d5a-8596-2e24c290d3c1.png)

![image](https://user-images.githubusercontent.com/108650199/196875916-7ff8c42e-de92-4d95-84bc-894eb0dd18db.png)


## 5-C. Motion Estimation
- LiDAR motion은 sweep 동안 일정한 각도 및 선형 속도로 모델링
  - 이를 통해 서로 다른 시간에 수신된 point에 대한 sweep내에서 포즈 변환을 선형 보간할 수 있음
  - 선형 보간 ? 두 지점 사이의 값을 추정할 때 그 값을 두 지점과의 직선거리에 따라 선형적으로 결정하는 방법
- t를 현재 timestamp라고 하면 tk+1이 sweep k+1의 시작 시간
  - ![image](https://user-images.githubusercontent.com/108650199/198938676-f9fae405-265f-4f68-8387-cc778539881a.png) 은 [tk+1, tk] 사이의 lidar pose transform 
    - ![image](https://user-images.githubusercontent.com/108650199/198938850-59a38524-0ed3-4c0f-9a72-8b6dcfea8867.png)
    - 여기서, L : Lidar 좌표계, txyz : LiDAR 좌표계에서의 translation thetaxyz : LiDAR 좌표계에서 rotation (오른손 법칙)

![image](https://user-images.githubusercontent.com/108650199/198939097-bff03000-5e07-4aef-9a73-c6a46c9bc6a4.png)

![image](https://user-images.githubusercontent.com/108650199/198939114-854df69f-5a1d-4345-9f12-a6b2f66c84d3.png)

![image](https://user-images.githubusercontent.com/108650199/198939143-a7c003f0-27d6-43f4-a12d-15d233910b1f.png)

## 5-D. Lidar Odometry Algorithm

![image](https://user-images.githubusercontent.com/108650199/198939155-4fbade7f-946b-4da4-b8e0-7ab2c359429f.png)

## 6. LiDAR Mapping
- Mapping 알고리즘은 odometry 측정 알고리즘보다 낮은 주파수에서 실행되며 sweep당 한 번만 호출
  - 즉, mapping 알고리즘은 10hz로 수행되는 odometry 알고리즘과 달리 매 sweep이 끝나는 순간(1hz 주기)에 한 번만 수행 
- sweep k+1이 끝나면 LiDAR odometry 측정은 왜곡되지 않은 포인트 클라우드 p-k+1과 동시에 [tk+1,tk+2]사이의 sweep 동안 lidar 모션을 포함하는 pose변환 TLk+1을 생
- 이 때, mapping 알고리즘은 p-k와 월드좌표계 W 사이를 매칭하는 역할을 수행 
 
![image](https://user-images.githubusercontent.com/108650199/202938933-d0ffdfe8-4bfb-4a92-9ced-c190fee17971.png)

![image](https://user-images.githubusercontent.com/108650199/202938940-15418a39-9b7d-4b43-92b0-57f5ed1bb1ae.png)

![image](https://user-images.githubusercontent.com/108650199/202938943-11f63e0f-738b-44fd-87e9-18bc6e7dedb1.png)

![image](https://user-images.githubusercontent.com/108650199/202940097-d5cc09ed-5e00-4288-850a-ebf19c5a5fa4.png)

![image](https://user-images.githubusercontent.com/108650199/202940130-c0ab7c72-7064-4eb5-be78-39ddf58c6cf5.png)

- feature point를 추출하는 방법은 odometry와 동일하나 odometry보다 10배 많은 feature를 추출
  - 1hz의 의 속도로 계산이 수행되므로 
- correspondence를 찾기 위해 현재 위치에서 반경 10m의 포인트클라우드만 사용

![image](https://user-images.githubusercontent.com/108650199/202941083-832bb0a6-e018-4106-88c2-a1a5943b82aa.png)

![image](https://user-images.githubusercontent.com/108650199/202941189-eb7f1133-23e2-488f-8a55-a0bbb876c73d.png)
