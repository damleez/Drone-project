LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain
===

## Abstract
- 지상 차량을 이용한 실시간 6자유도 포즈 추정을 위해 경량의 ground-optimized인 lidar odometry 및 mapping method인 Lego-LOAM을 제안
- LegO-LOAM은 저전력 임베디드 시스템에서 실시간 포즈 추정을 달성할 수 있기 때문에 가벼움
- LeGO-LOAM은 segementation과 optimization 단계에서 ground plane의 존재를 활용하므로 ground-optimizied 됐음
- 먼저 포인트 클라우드 segmentation을 적용하여 노이즈를 필터링하고 특징 추출을 적용하여 독특한 평면 및 에지 특징을 얻음
- 2단계 Levenberg-Marquardt 최적화 방법은 평면 및 에지 feature을 사용하여 연속 스캔에서 6자유도 변환의 서로 다른 구성요소를 해결
- LOAM과 비교시 LegO-LOAM이 감소된 계산 비용으로 유사하거나 더 나은 정확도를 달성함을 보여줌
- 또한 LeGO-LOAM을 SLAM 프레임워크에 통합하여 KITTI 데이터 세트를 사용하여 테스트한 드리프트로 인한 포즈 추정 오류를 제거

## Introduction (동향 위주 - 안봐도 됨)
### - 기본이론 (왜 라이다를 쓰는지)
- 지능형 로봇의 기능 중 지도 작성 및 상태 추정은 가장 기본적인 전제 조건
- 비전 기반 및 라이더 기반 방법을 사용하여 실시간 6자유도 동시 위치 파악 및 매핑(SLAM)을 달성하기 위해 많은 노력을 기울였음
  - loop closure에 대한 장점, but 조명 및 시점 변경에 대한 민감도 ↑
- But, 라이더 기반 방법은 밤에도 작동하며 많은 3D 라이더의 고해상도 덕분에 넓은 조리개를 통해 원거리에서 환경의 미세한 부분까지 캡처
---
### - 라이다 tf찾는 방식
- 두 개의 라이더 스캔 간의 변환을 찾는 일반적인 접근 방식은 ICP(반복 근접점)
- 포인트 단위 수준에서 대응을 찾아 ICP는 중지 기준이 충족될 때까지 두 세트의 포인트를 반복적으로 정렬
- 스캔에 많은 양의 포인트가 포함된 경우 ICP에 엄청난 계산 비용이 발생
- ICP의 효율성과 정확도를 향상시키기 위해 많은 변형이 제안
  - point-to-plane or G-CIP ...
---
### - 특징점 기반 매칭 방식
- Feature-based matching 방법은 환경에서 대표 특징을 추출하여 계산 자원을 덜 필요로 하기 때문에 더 많은 관심
- 이러한 기능은 효과적인 일치와 관점의 불변에 적합해야 함
- Point Feature Histograms(PFH) 및 Viewpoint Feature Histograms(VFH)과 같은 많은 검출기가 간단하고 효율적인 기술을 사용하여 포인트 클라우드에서 이러한 특징을 추출하기 위해 제안
- Kanade-Tomasi 모서리 검출기를 사용하여 포인트 클라우드에서 범용 특징을 추출하는 방법 소개
- 조밀한 점 구름에서 선 및 평면 특징을 추출하는 프레임워크도 소개
---
### - pointcloud registration
- 방법 1. 로컬 클러스터에서 점 곡률 계산을 수행하는 키포인트 선택 알고리즘을 제시도 함
  - 그런 다음 선택한 키포인트를 사용하여 일치 및 장소 인식을 수행
- 방법 2. 점 구름을 범위 이미지에 투영하고 깊이 값의 2차 도함수를 분석하여 일치 및 장소 인식을 위해 곡률이 높은 점에서 특징을 선택
- 방법 3. 환경이 평면으로 구성되어 있다고 가정하고 평면 기반 등록 알고리즘을 제안
  - 숲과 같은 실외 환경은 이러한 방법의 적용을 제한할 수 있음
- 방법 4. Velodyne 라이더를 위해 특별히 설계된 칼라 라인 세그먼트(CLS) 방법이 제시
  - CLS는 스캔의 두 연속 "링"에서 점을 사용하여 무작위로 선을 생성
  - 따라서 두 개의 라인 클라우드가 생성되어 등록에 사용
  - 그러나 이 방법은 선이 무작위로 생성되는 문제
- 방법 5. 분할 기반 등록 알고리즘 제안
  - SegMatch는 먼저 포인트 클라우드에 세분화를 적용
  - 그런 다음 고유값과 모양 히스토그램을 기반으로 각 세그먼트에 대해 특징 벡터가 계산
  - 랜덤 포레스트는 두 스캔의 세그먼트를 일치시키는 데 사용
  - 이 방법은 온라인 포즈 추정에 사용할 수 있지만 약 1Hz에서만 현지화 업데이트를 제공
---
### - LOAM
- low-drift 및 real-time LOAM(lidar odometry and mapping) 방법이 제안
- LOAM은 스캔 간의 대응 관계를 찾기 위해 에지/평면 스캔 일치에 대한 포인트 기능을 수행
- 특징은 로컬 영역에서 점의 거칠기(roughness)를 계산하여 추출
  - 거칠기 값이 높은 포인트가 가장자리 피처로 선택 > 이거 C값 큰게 edge 작은게 planar얘기하는 듯
  - 마찬가지로 거칠기 값이 낮은 점은 평면 형상으로 지정
- 실시간 성능은 추정 문제를 두 개의 개별 알고리즘으로 새롭게 분할하여 달성 > LOAM도 odometry랑 mapping 병렬로 실행
- odometry : 한 알고리즘은 높은 주파수에서 실행되고 낮은 정확도로 센서 속도를 추정
- mapping : 다른 알고리즘은 저주파에서 실행되지만 고정밀 모션 추정을 반환
- 두 추정값은 함께 융합되어 고주파수와 고정확도 모두에서 단일 동작 추정값을 생성
---
### - 🌟️ This paper proposed, ~ 🌟️
- 소규모 임베디드 시스템에서 효율적으로 구현할 수 있는 방식으로 3D 라이더가 장착된 지상 차량에 대한 신뢰할 수 있는 실시간 6자유도 포즈 추정을 추구
#### - 어려운 점
- 많은 무인 지상 차량(UGV)에는 제한된 크기로 인해 서스펜션이나 강력한 계산 장치가 없음
- 가변 지형을 주행하는 소형 UGV는 매끄럽지 않은 움직임을 자주 접하게 되며, 그 결과 획득한 데이터가 왜곡되는 경우가 많음
- 중첩(overlap)이 제한된 large motion으로 인해 두 개의 연속 스캔 간에 신뢰할 수 있는 feature correspondences을 찾기도 어려움
- 그 외에도 3D 라이더에서 수신한 많은 양의 포인트는 제한된 온보드 계산 리소스를 사용하여 실시간 처리에 어려움을 야기
#### - 이러한 UGV에 LOAM을 구현할 시 문제점
- 이러한 작업에 대해 LOAM을 구현하면 UGV가 부드러운 동작과 안정적인 기능으로 작동되고 충분한 계산 리소스가 지원될 때 낮은 드리프트 동작 추정을 얻을 수 있음
- 그러나 리소스가 제한되면 LOAM의 성능이 저하
- 왜냐하면, pointcloud에서 모든 point의 c(거칠기)를 계산해야해서 경량 임베디드 시스템의 feature extraction 업데이트 빈도가 센서 업데이트 빈도를 마냥 계속 따라갈 수가 없음
- noisy 환경에서 UGV를 작동하는 것도 LOAM에 문제
  - 라이다의 장착 위치는 종종 소형 UGV의 지면에 가깝기 때문에 지면에서 센서 노이즈가 지속적으로 존재할 수 있기 때문 
- 예를 들어 잔디에서 범위를 반환하면 거칠기 값(c)이 높을 수 있음 > planar인데 edge가 됨
  - 결과적으로 이러한 점에서 신뢰할 수 없는 edge 특징이 추출될 수 있음
- 유사하게, edge 또는 평면 feature은 나무 잎에서 반환된 점에서 추출될 수도 있음
- 이러한 기능은 일반적으로 스캔 일치에 대해 신뢰할 수 없음 왜냐하면 동일한 잔디 잎이나 잎이 두 번의 연속 스캔에 표시되지 않을수도 있음
- 이러한 기능을 사용하면 부정확한 등록과 큰 드리프트가 발생
#### - 🧠️ 다시 정리하자면 🧠
- LOAM은 모든 point에 대해 c계산해야 돼서 리소스 제한되면 오류 발생 > 업데이트를 못따라감
- noisy 환경에서도 문제 > UGV 땅에 가까움, 신뢰할 수 없는 edge/planar feature extration
- 결과적으로 부정확한 registration과 큰 drift 발생
 #### - 💥️ 따라서, LeGO-LOAM의 제안 💥️
- 다양한 지형이 있는 복잡한 환경에서 UGV의 자세 추정을 위한 lightweight와 ground-optimized LOAM(LeGO-LOAM)을 제안
- Point cloud segmentation은 ground 분리 후 신뢰할 수 없는 기능을 나타낼 수 있는 포인트를 삭제하기 위해 수행
- LegO-LOAM은 또한 포즈 추정을 위한 2단계 최적화를 도입하기 때문에 지상에 최적화(ground-optimized)
  - 지면에서 추출한 평면 형상을 사용하여 첫 번째 단계에서 z, roll, pitch 구함
  - 두 번째 단계에서 나머지 변환 x, y, yaw는 segmentation된 포인트 클라우드에서 추출한 edge 특징을 일치시켜 얻음
- 또한 모션 추정 드리프트를 수정하기 위해 루프 폐쇄를 수행하는 기능을 통합
#### - 🧠️ 다시 정리하자면 🧠
- 다양한 지형과 복잡한 noisy한 환경을 고려하고자 함
- 그에 따른 ground-optimized LOAM제안하며 2단계로 구성
- ground-optimized할 때, pointcloud segementation하는데 신뢰x 포인트 삭제하고, edge특징 추출시켜 x,y,yaw얻음 
  - 평면은 z, roll, pitch 쓰는데 당연한거임 생각해보셈  

## 2. System Hardware
- Velodyne VLP-16 및 HDL-64E 3D 라이더에서 수집한 데이터 세트를 사용하여 검증
### - VLP-16 spec
- VLP-16 측정 범위는 ± 3cm의 정확도로 최대 100m
- 수직 시야각(FOV)은 30º(±15º)이고 수평 FOV는 360º
- 16채널 센서는 2º의 수직 각 분해능을 제공
- 수평 각도 분해능은 회전 속도에 따라 0.1 ~ 0.4º까지 다양
---
- 👉️ 논문 전체에서 우리는 0.2 ◦의 수평 각 해상도를 제공하는 10Hz의 스캔 속도를 선택
---
### - HDL-64E
- HDL-64E도 수평 FOV가 360°이지만 VLP-16보다 48개 더 많은 채널
- HDL-64E의 수직 FOV는 26.9º
---
- 👉️ 본 논문에서 사용된 UGV는 Clearpath Jackal이며 저렴 IMU도 있음

## 3. Lightweight LiDAR Odometry and Mapping
### 3-A. System overview

![image](https://user-images.githubusercontent.com/108650199/202979052-07293406-68f8-425a-bc5a-5fca371c4c84.png)

- input : 3D lidar
- output : 6-DOF pose estimation
- 첫 번째, segmentation은 단일 스캔의 포인트 클라우드를 가져와 segmentation을 위해 범위 이미지에 투영
- 두 번째, segmentation된 point cloud가 feature extration module로 전송
- 세 번째, LiDAR odometry는 이전 module에서 추출한 feature을 사용하여 연속 scan과 관련된 transformation을 찾음
  - feature은 global point cloud map에 register하는 lidar mapping에서 추가 처리
- 마지막, 변환 통합 모듈은 라이더 주행 거리 측정과 라이더 매핑의 포즈 추정 결과를 융합하고 최종 포즈 추정을 출력

#### - 🧠️ 다시 정리하자면 🧠
- Segmentation 모듈에서 Point Cloud → Range Image → Segmented Point의 과정을 진행
- Feature Extration 모듈에서 Segmented Cloud로 부터 Feature를 뽑고 이 feature를 이용해 odometry와 mapping
- 두 pose를 integration


### 3-B. Segmentation

> Fig

![image](https://user-images.githubusercontent.com/108650199/202981596-b5673406-db5c-411c-89c7-99ff44d22597.png)

- (a) : origin
- (b) : 빨간 색 점은 ground points이며, 나머지 점은 segmentation 이후 남은 점 
- (c) : 파란색과 노란색 점은 edge 및 planar feature
- (d) : 초록색과 핑크색 점은 edge와 planar의 feature

---
#### - Range image 변환
- Pt = {p1 , p2 , ..., pn }을 시간 t에서 획득한 포인트 클라우드라고 하고, 여기서 pi는 Pt의 한 점
- Pt는 먼저 범위 이미지(range image)에 투영
  - VLP-16의 수평 및 수직 각 해상도가 각각 0.2º와 2º이기 때문에 투영된 범위 이미지의 해상도는 1800 x 16
- Pt의 각 유효한 점 pi는 이제 범위 이미지에서 고유한 픽셀로 표시
- pi와 관련된 범위 값 ri는 해당 지점 pi에서 센서까지의 유클리드 거리를 나타냄
- 경사진 지형은 많은 환경에서 일반적이므로 지면이 평평하다고 가정하지 않음
#### - Ground plane estimation
- ground plane estimation으로 볼 수 있는 range image의 column-wise 평가는 분할 이전의 그라운드 포인트 추출을 위해 수행
- 이 과정이 끝나면 그라운드를 나타낼 수 있는 포인트는 그라운드 포인트로 레이블이 지정되며 분할에 사용되지 않음
#### - Image-based segmentation 
- image-based segmentation을 range image에 적용하여 포인트를 많은 클러스터로 그룹화
- 동일한 클러스터의 포인트에는 고유한 레이블이 할당
  - 그라운드 포인트는 특별한 유형의 클러스터
- 포인트 클라우드에 segmentation를 적용하면 처리 효율성과 특징 추출 정확도를 향상
- 위에서도 말했지만, 로봇이 noisy 환경에서 작동한다고 가정하면 나무 잎과 같은 작은 물체는 두 번의 연속 스캔에서 동일한 잎을 볼 수 없기 때문에 사소하고 신뢰할 수 없는 특징을 형성할 수 있음
  - 🌟️ 해결 방안 : 분할된 포인트 클라우드를 사용하여 빠르고 안정적인 특징 추출을 수행하기 위해 30개 미만의 포인트가 있는 클러스터를 생략 🌟️
- 위의 Fig를 보면 분할 전후의 포인트 클라우드의 시각화
- 원래 포인트 클라우드에는 신뢰할 수 없는 기능을 생성할 수 있는 주변 식물에서 얻은 많은 포인트가 포함 > 분할된 pc이용, threshold 지정
  - 이로인해, large object(like 나무 줄기 no 잎)과 ground point만 추가 처리를 위해 보존
  - 동시에 이러한 포인트만range image에 저장
- 우리는 또한 각 점에 대해 세 가지 속성을 얻음
  - (1) ground point 또는 segmentation point으로서의 레이블
  - (2) Range image의 열 및 행 인덱스
  - (3) Range value(범위 값) 이러한 속성은 다음 모듈에서 활용

#### - [🐸️ 남의 것 🐸️ Segmentation의 이해](https://dreambreaker-ds.tistory.com/entry/LOAM-Lego-LOAM#4d0d24a1-b154-41f7-904a-5b18e6efd76f)
- Point Cloud (그림 a)를 1800×16 크기의 range image로 변환하고 range image에서의 pixel value는 point의 sensor로 부터의 euclidean distance로 함
- 그리고 Segmentation 이전에 ground extration을 하고서 segmentation을 한다고 하는데 이 ground extration은 column-wise evaluation방법을 통해서 한다고 함
  - 이 방법은 range image 상에서 column-wise slope를 이용해서 threshold 미만이면 ground로 판단해서 ground point들을 뽑아냄
- 이렇게 ground point들의 index를 제외한 나머지 range image에서 image-based segmentation
- 여기서 robustness를 위해 point 갯수가 30개 미만인 segment들은 사용하지 않음
- 이 결과로 segmented point과 groud point (그림 b)를 얻음

### 3-C. Feature Extraction

![image](https://user-images.githubusercontent.com/108650199/202986336-e9fd7159-15ca-424a-9844-9fa2d9486bc3.png)

#### - 거칠기 구하기
- 앞에서 추출한 segmented point와 ground point에서 feature를 뽑는 과정
- LOAM에서와 같은 smoothness를 정의해서 사용
  - 다른점은 LOAM은 origin point cloud에서 feature 뽑고, 얘는 ground point와 segmentation point에서 뽑는거 

![image](https://user-images.githubusercontent.com/108650199/202986365-f7b59366-da57-4299-b46c-6987e35b290a.png)

- S를 range image의 동일한 행에 있는 연속적인 pi의 점 집합이라고 함
  - S에 있는 점의 절반은 pi 의 양쪽에 있음
- 이 논문에서는 |S| ~ 10이라고 set
  - 앞 뒤로 5개의 point사용한다는 뜻
- 분할 중에 계산된 범위 값을 사용하여 S에서 점 pi의 거칠기를 평가
---
#### - 거칠기를 이용한 Feature extration
- 모든 방향에서 특징을 고르게 추출하기 위해 range image를 수평으로 여러 개의 동일한 하위 이미지로 나눔
- 거칠기 값 c에 따라 하위 이미지의 각 행에 있는 점을 정렬
- LOAM과 유사하게 임계값 c th를 사용하여 다양한 유형의 기능을 구별
- c가 c번째 에지 피처보다 큰 점과 c가 c번째 평면 피처보다 작은 점을 호출
- edge feature points : 그런 다음 지면에 속하지 않는 최대 c를 갖는 nFe edge feature points 을 서브 이미지의 각 행에서 선택
- planar feature points : nFp 최소 c를 갖는 평면 특징점(지면 또는 분할 점으로 레이블이 지정될 수 있음)은 동일한 방식으로 선택

![image](https://user-images.githubusercontent.com/108650199/202990266-b39b24a3-90c8-492d-8014-37d5f15efb82.png)

  - 글씨체 따라 집합이 달라지므로 캡쳐본으로 대신함

### 3-D. LiDAR Odometry
- LiDAR odometry module은 연속적인 스캔 사이의 sensor motion을 추정
- 두 스캔간의 tf는 point-to-edge, point-to-plane 스캔 매칭을 수행함
- 다시말해, 이전 스캔의 ![image](https://user-images.githubusercontent.com/108650199/202992439-4c7e2a1b-1f4c-476a-aee8-f0a560d9c315.png)
와 같은 특징점 세트로부터 ![image](https://user-images.githubusercontent.com/108650199/202992357-71d7599b-8a7c-4658-8486-996da19455b2.png) 의 점에 대한 feature correspondences를 찾아야 함
- feature 매칭 정확도와 효율성을 개선하기 위해 몇가지 변경 사항을 적용할 수 있음
#### - 1) Label matching
- ![image](https://user-images.githubusercontent.com/108650199/202993288-24c1044f-19c2-4f3c-b977-7d6c4a497564.png) 의 각 기능은 분할 후, 레이블로 인코딩 되므로 ![image](https://user-images.githubusercontent.com/108650199/202993411-7cdddc9f-cf27-429f-92d4-8ebd5fab04e9.png) ![image](https://user-images.githubusercontent.com/108650199/202993444-792058ac-f53e-48f7-af1d-d5581eafa192.png) 에서 동일한 레이블을 가진 correspondence만 찾음
- ![image](https://user-images.githubusercontent.com/108650199/202993764-fa91f0c9-9a51-4e43-9805-d3535ae80245.png) 의 경우, ![image](https://user-images.githubusercontent.com/108650199/202994126-2f522f49-da93-4401-9084-6db22af8397e.png) 에서 ground point로 레이블이 지정된 점만 해당 correspondence로 planar patch를 찾는데 사용
- ![image](https://user-images.githubusercontent.com/108650199/202994568-4733641b-9d4c-4e71-a523-0bb62c3f96f6.png) 의 edge feature에 대해 해당 edge line은 분할된 클러스터의 ![image](https://user-images.githubusercontent.com/108650199/202994640-1f38b496-1a15-4483-8474-1ef8b651b431.png) 에서 발견
- 이러한  방식으로 correspondence를 찾는 것은 일치 정확도를 향상
- 다시 말해, 동일한 객체에 대한 일치하는 correspondence는 두 스캔 사이에서 발견될 확률이 높음
- 이 프로세스는 잠재적인 대응관계 후보의 범위를 좁힘


#### - 2) Two-step L-M optimization 
- 현재 스캔의 에지와 평면 특징점 사이의 거리에 대한 일련의 non-linear 표현식과 이전 스캔의 대응 관계가 하나의 포괄적인 거리 벡터로 컴파일됨
- Levenberg-Marquardt(L-M) 방법은 두 개의 연속 스캔 사이의 최소 거리 변환을 찾는 데 적용
- 여기서는 2단계 L-M 최적화 방법을 소개
- 최적의 변환 T는 다음 두 단계에서 찾을 수 있음
#### - 1) z, roll, pitch
- z, roll, pitch는 ![image](https://user-images.githubusercontent.com/108650199/202995506-f86d3fb0-e58f-4adc-a027-71d5bad1f122.png) 의 평면 특징과 ![image](https://user-images.githubusercontent.com/108650199/202995624-df1ca586-5c15-4445-bf7b-d32017b4934e.png) 의 대응 관계를 일치시켜 추정
#### - 2) x, y, yaw
- 나머지 x, y, yaw는 z, roll, pitch를 제약조건으로 사용하여 edge feature F와 Ft-1의 해당 대응을 사용하여 추정
- x, y, yaw는 첫번째 최적화에서도 얻을 수 있지만 정확도가 떨어지고 두 번째 단계에서는 사용되지 않음
#### - 3) 마지막으로
- z, roll, pitch 및 x, y, yaw를 융합하여 두 개의 연속 스캔 간의 6D 변환을 찾음
- 제안된 2단계 최적화 방법을 사용하여 계산 시간이 약 35% 감소하면서 유사한 정확도를 얻을 수 있음을 관찰

#### - 🧠️ 다시 정리하자면 🧠

![image](https://user-images.githubusercontent.com/108650199/202996370-81e2667a-768b-44a3-b26b-ce021a94d87f.png)

![image](https://user-images.githubusercontent.com/108650199/202996416-0e53695a-a2ee-49cc-9dff-454a3abd072a.png)

### 3-E. LiDAR Mapping
- LiDAR mapping module은 ![image](https://user-images.githubusercontent.com/108650199/202997387-eaa5b246-a4c7-42cc-8ccc-b18bf29e46e0.png) 의 feature를 근처 pointcloud map ![image](https://user-images.githubusercontent.com/108650199/202997599-e7ad7426-803c-4bcf-a103-2bef0ab399b2.png) 과 matching하고 refine한 pose tf를 얻음
  - 하지만, lower frequency에서 작동 중 
- 그리고나서, L-M 방법을사용해서 최종적인 pose를 얻음
- LeGO-LOAM의 LOAM과의 main 차이점은 마지막 pointcloud map이 어떻게 저장되느냐 ? 이다
  - 단 하나의 point cloud map을 저장하는 것 대신에, 개별 feature set ![image](https://user-images.githubusercontent.com/108650199/202998426-55c36f2f-da95-438a-ae05-dafe9c5afe08.png) 을 저장
- ![image](https://user-images.githubusercontent.com/108650199/202998553-78873222-e69f-4ecb-ba46-e74325835534.png) 라고 하면 라고 하면 각각의 Mt에 대응하는 pose를 연결짓는 식으로 저장
- ![image](https://user-images.githubusercontent.com/108650199/202998886-be1f1efa-5cca-45ec-b5f1-711773d5789b.png) 
가 있음

![image](https://user-images.githubusercontent.com/108650199/202998939-5aa5dfaa-c08e-418e-bc64-076080923064.png)
